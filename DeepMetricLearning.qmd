---
title: "Deep metric learning"
subtitle: "conceitos, aplicaÃ§Ãµes e algumas contribuiÃ§Ãµes"
author: Heitor S. Ramos <br> <a href="mailto:heitor@dcc.ufmg.br">heitor@dcc.ufmg.br</a>
# date: 03/08/2022 
# abstract-title: 
format: 
  revealjs: 
    code-fold: true 
    execute: 
      enabled: true
    echo: true
    jupyter: python3
    slide-number: true
    theme: default
    controls: true
    progress: true
    smaller: true
    # scrollable: true
    chalkboard:
      buttons: true
      chalk-width: 1
      theme: whiteboard
    preview-links: auto
    logo:  common/logos.png
    include-in-header:
    - text: |
        <style>
        .reveal .slide-logo {
          max-height: unset;
          height: 100px;
        }
        </style>
    style: common/style.css
    footer: <https://heitorramos.github.io/>
# search: true
# resources:
#   - aula06.pdf
---

## Artificial Intelligence
Artificial intelligence (sometimes referred to by the acronym AI) is the science that studies the modeling of human-like intelligence exhibited by machines or software.

## Artificial Intelligence

![](figs/IA.png)

## Artificial Intelligence (Deep Learning Book)

![](figs/deep_learning-IA.png){#ia fig-align="center"}

## Classification
<!-- :::{.columns}

:::{.column width="50%"} -->
a subcategory of supervised learning where a function maps the input to an output (discrete label) based on examples of input-output pairs.

<!-- :::

:::{.column width="50%"} -->
![](figs/dog.png){#dog fig-align="center" width="70%"}
<!-- :::
 
::: -->
 
## Classification

![](figs/laranja_maca.png){#laranja fig-align="center"}

## Feature


- A feature is a characteristic that describes an object. 

- Any attribute of an object can be treated as a feature, whether it is a number, text, date, boolean, etc

![](figs/features.png){#features fig-align="center"}

## Features and Classification

![](figs/features_classification.png){#features_class fig-align="center"}
 

## Supervised Learning

is a machine learning task that involves mapping inputs to outputs based on examples of input-output pairs.

![](figs/supervised.png){#supervised fig-align="center"}



## What if?

:::{.callout-important}
Our model learn the features?
:::

![](figs/deep-learning-model.png){#deeplearningmodel fig-align="center"}

## We want more than representation learning

![](figs/metric_learning.png)

## Different notions of similarity
 

![](figs/different_similarities.png){#similaries fig-align="center" width="80%"}

## Metric Space

Is a non-empty set together with a notion of distance between its elements. The distance is measured by a function called **metric** or distance **function**.


## Formally, a metric space is

an ordered pair $(ğ‘€,ğ‘‘)$, where $ğ‘€$ is a set and $d$ is a metric on $ğ‘€$, i.e., a function $d: M\times M \rightarrow \mathbb R$, satisfying for all points $x,y \in M$

1. The distance from a point to itself is zero: $ğ‘‘(ğ‘¥,ğ‘¥)=0$
2. (positivity) the distance between two points is always positive: if $ğ‘¥\ne ğ‘¦$ then $ğ‘‘(ğ‘¥,ğ‘¦)>0$
3. (Symmetry) the distance from $ğ‘¥$ to $ğ‘¦$ is always the same as the distance from $ğ‘¦$ to $ğ‘¥$: $ğ‘‘(ğ‘¥,ğ‘¦)=ğ‘‘(ğ‘¦,ğ‘¥)$
4. The triangle innequality holds: $ğ‘‘(ğ‘¥,ğ‘§)\leq ğ‘‘(ğ‘¥,ğ‘¦)+ğ‘‘(ğ‘¦,ğ‘§)$

## Metric examples

![](figs/metric_examples.png){#metrics fig-align="center" width="50%"}

## Distance notion

:::{.panel-tabset}

### What is extreme?
![](figs/pressao2.png){#pressao fig-align="center" width="50%"}


<!-- ## Distance notion -->

### Not Euclidean

![](figs/distancia_pressao2.png){#pressao2 fig-align="center" width="50%"}

<!-- ## Equidistant points -->
### Equidistant

![](figs/distancia_centro.png){#equidistant fig-align="center" width="50%"}

<!-- ## Equidistant points -->

### Equidistant 2

![](figs/elipse.png){#equidistant2 fig-align="center" width="50%"}

<!-- ## Equidistant points -->

### Equidistant 3

![](figs/distancias.png){#equidistant3 fig-align="center" width="50%"}

:::

## What distance are we talking about?

:::{.incremental}
:::{.callout-important}
- Mahalanobis
:::

- $$D(\boldsymbol x, \boldsymbol y) = \sqrt{(\boldsymbol x-\boldsymbol y)^\top \Sigma^{-1} (\boldsymbol x - \boldsymbol y)}$$
:::
## A little  linear algebra

- Remember that $\Sigma$ is a **semi**-positive matrix
- We can rearrange $\Sigma^{-1} = W^\top W$
- and rearrange the Mahalanobis distance as $$D(\boldsymbol x, \boldsymbol y) = \Vert W\boldsymbol x- W \boldsymbol y\Vert_2$$

![](figs/transformacaoLinear.png){#linear fig-align="center"}

## Metric Learning main goal

- Learn semantic relations between data points

- Ideia: Learn a mapping function that:
  - Semantic relation $\rightarrow$ (Pseudo) Metric distances

![](figs/dml_goal.png){#goal fig-align="center" width="60%"}

## In a nutshell

- Step 1 - Choose a parameterized embedding function $\Psi(\cdot)$

- Step 2 - Pick a distance measure $\Delta$ for the embedding space, e.g. $\Delta(\Psi(x, y)) \rightarrow$ Euclidean distance

- Step 3 - Define data $D$ and similarity rules:
  - $S = \{(x, y) \mid x, y \text{ are similar}\}$

  - $D = \{(x, y) \mid x, y \text{ are dissimilar}\}$

  - $T = \{(x, y, z) \mid x  \text{ is more similar to }  y \text{ than to }z\}$

## In a nutshell

![](figs/schemaDM.png){#schema fig-align="center" width="90%"}

<!-- ## CrÃ©ditos
:::{.callout-important}
Os slides desse curso sÃ£o fortemente baseados no curso do FabrÃ­cio Murai e do Erickson Nascimento
::: -->

<!-- ## Objetivos de Aprendizagem{.scrollable}

1. Conhecer a definiÃ§Ã£o de *embedding* se algumas aplicaÃ§Ãµes
2. Saber calcular normas vetoriais 1,2, $\infty$, $p$ e $S$ (onde $S$ Ã© uma matriz definida positiva)
3. Conhecer as propriedades das normas vetoriais
4. Saber normalizar um vetor
5. Conhecer a geometria das normas vetoriais 

## ReferÃªncias
- Wikipedia
    - [Normas vetoriais](https://en.wikipedia.org/wiki/Norm_(mathematics))
    - [Normas matriciais](https://en.wikipedia.org/wiki/Norm_(mathematics))
- Youtube
    - [Chad Higdon-Topaz](https://en.wikipedia.org/wiki/Norm_(mathematics))

## Onde usar normas vetoriais?{.scrollable}

- Normas vetoriais sÃ£o associadas a **tamanhos** e a **distÃ¢ncias**
- Existem vÃ¡rias aplicaÃ§Ãµes em que queremos medir a (dis-)similaridade entre objetos. Em geral, isso Ã© feito em 2 passos:
    - Passo 1: representar objetos como vetores. Estes vetores sÃ£o conhecidos como ***embeddings***
    - Passo 2: calcular distÃ¢ncia entre ***embeddings***

## Embeddings (representaÃ§Ã£o vetorial)
![Vetores com 26 dimensÃµes projetados no â€œmelhorâ€ espaÃ§o de dimensÃ£o 2](figs/Aula05/embeddings_song.png)

::: aside 
Almeida, M.A.D., Vieira, C.C., Melo, P.O.S.V.D. and AssunÃ§Ã£o, R.M., 2019. Random Playlists Smoothly Commuting Between Styles. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 15(4), pp.1-20.
:::

## Embeddings (representaÃ§Ã£o vetorial) {.scrollable}
![Objetivo: gerar playlists que passem de um estilo a outro de forma SUAVE.
Playlists devem ser aleatÃ³rias, diferentes a cada dia.](figs/Aula05/embeddings_songs_2.png)

<!-- ::: aside 
Almeida, M.A.D., Vieira, C.C., Melo, P.O.S.V.D. and AssunÃ§Ã£o, R.M., 2019. Random Playlists Smoothly Commuting Between Styles. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 15(4), pp.1-20.
::: -->

<!-- ## Norma de vetor em $\mathbb R^2$ e $\mathbb R^3$

![Normas em $\mathbb R^2$ e $\mathbb R^3$](figs/Aula05/norma_r2_r3.png)

## Norma de vetor em  $\mathbb R^n$

$$x = (x_1,x_2,\ldots,x_n)^\top$$ 

$$\Vert x \Vert = \sqrt{x_1^2 + x_2^2 + \ldots + x_n^n}$$

$$\Vert x \Vert = \sqrt{x^\top x}$$


## Exemplo {.scrollable}

$$u = \begin{bmatrix}0\\-1\\2\\-2\\4\end{bmatrix} \text{ e } v = \begin{bmatrix}i\\2\\1+i\\0\\1-i\end{bmatrix}$$

$$\Vert u \Vert = \sqrt{u^\top u} = \sqrt{0+1+4+4+16} = 5 $$

$$\Vert v \Vert = \sqrt{v^* v} = \sqrt{1+4+2+0+2} = 3 $$

## Propriedades{.scrollable}
- A definiÃ§Ã£o de norma euclidiana garante que para todos os escalares $\alpha$

$$\Vert x\Vert \ge 0 \text{, } \Vert x\Vert = 0 \Leftrightarrow x =0 \text{ e } \Vert \alpha x\Vert = \vert \alpha\vert \, \Vert x \Vert$$

:::{.callout-important}
## NormalizaÃ§Ã£o do vetor
Dado um vetor $x \ne 0$, Ã© frequentemente conveniente termos outro vetor que aponta para a mesma direÃ§Ã£o que $x$ mas apresenta comprimento unitÃ¡rio. Para isso, **normalizamos** o vetor

$$\lVert u\rVert = \left\lVert\frac{x}{\lVert x\rVert} \right\rVert= \frac{1}{\lVert x\rVert} \lVert x\rVert = 1$$

:::

## IntuiÃ§Ã£o do produto interno

:::{.columns}

:::{.column width="70%"}
$$ a^\top b = \Vert a \Vert \Vert b\Vert \cos(\theta)$$

O comprimento da projeÃ§Ã£o ortogonal de um vetor $a$ sobre o vetor $b$ Ã©

$$ \Vert a\Vert \cos(\theta) = a^\top \frac{b}{\Vert b\Vert}$$
:::

:::{.column width="30%"}
![](figs/Aula05/projecao2.svg.png)
:::

:::

## Desigualdade de Cauchy-Schwarz {.scrollable}

$$ | x^\top y| = \Vert x \Vert \Vert y \Vert |\cos(\theta)| \le \Vert x \Vert \Vert y \Vert$$


Teorema da Desigualdade Triangular

$$ \Vert x+y\Vert \le \Vert x \Vert + \Vert y\Vert \text{, para todo $x,y$}$$ 

vamos elevar os dois lados ao quadrado, temos
$$(\Vert x+y\Vert)^2 \le (\Vert x \Vert + \Vert y\Vert)^2 \le \Vert x\Vert^2 + 2\Vert x\Vert \Vert y\Vert + \Vert y\Vert^2$$
\begin{align}\Vert v\Vert^2 &= (\sqrt{v^\top v})^2\\ &= (x+y)^\top (x+y) \\ &= \Vert x\Vert^2 + 2x^\top y+\Vert y\Vert^2 \\ &\le \Vert x\Vert^2 + 2\vert x^\top y\vert + \Vert y\Vert^2 \\&\le \Vert x\Vert^2 + 2\Vert x\Vert \Vert y\Vert + \Vert y\Vert^2
\end{align}

## DistÃ¢ncia entre dois vetores {.scrollable}

\begin{align} d(p,q) = d(q,p) &= \sqrt{(q_1 - p_1)^2 + \ldots + (q_n - p_n)^2} \\ &= \sqrt{\sum_{i=1}^n (q_i-p_i)^2}\end{align}

- Caso Bidimensional 
![DistÃ¢ncia](figs/Aula06/dist_bidimensional.png){width="150%"}

## Outras distÃ¢ncias{.scrollable}
Manhattan distance (norma L1)

![Manhatan vs Euclidian](figs/Aula06/manhattan_distance.png){width="50%"}

## Norma L1

$$\Vert x \Vert_1 = \vert x_1\vert + \vert x_2 \vert + \ldots + \vert x_n\vert$$

Caso bi-dimensional:

$$\Vert (x,y) \Vert_1 = \vert x\vert + \vert y\vert$$

$$d(x,y) = \vert x_1 - y_1 \vert + \ldots + \vert x_n - y_n\vert$$

## Norma

- Quando algo pode ser considerado uma funÃ§Ã£o de distÃ¢ncia ou tamanho de vetores?
- Quando nÃ£o teremos contradiÃ§Ã£o?
- Que propriedades valem para uma norma (vÃ¡lida) qualquer?

:::{.callout}
MatemÃ¡ticos possuem uma definiÃ§Ã£o genÃ©rica de norma que vale tambÃ©m para outras situaÃ§Ãµes alÃ©m de matrizes
:::

## Norma: definiÃ§Ã£o

:::{.callout-note}
## Norma de um vetor (definiÃ§Ã£o geral)

A norma para um espaÃ§o vetorial $\mathcal V$ real ou complexo Ã© uma funÃ§Ã£o $\Vert \star \Vert$ que mapeia $\mathcal V$ em $\mathbb R$ e que satisfaz as seguintes condiÃ§Ãµes

- $\Vert x \Vert \ge 0 \text{ e } \Vert x \Vert =0 \Leftrightarrow x = 0$
- $\Vert \alpha x\Vert = \vert \alpha \vert \Vert x\Vert$ para todos escalares $\alpha$
- $\Vert x+y\Vert \le \Vert x \Vert + \Vert y \Vert$.

:::

## Norma L2 e Lp{.scrollable}

Generalizando:

- $\Vert x \Vert_1 = \sqrt[1]{\sum_{i=1}^n \vert x_i \vert^1}$

- $\Vert x \Vert_2 = \sqrt{\sum_{i=1}^n \vert x_i \vert^2}$

- $\Vert x \Vert_p = \sqrt[p]{\sum_{i=1}^n \vert x_i \vert^p}$

- $\Vert x \Vert_\infty = \max_i \vert x_i\vert$

:::{.callout-tip}
## Falhas

$p=0$ nÃ£o Ã© uma norma vÃ¡lida pois $\Vert 2v\Vert_0 = \Vert v \Vert_0$, que viola a propriedade da multiplicaÃ§Ã£o por um escalar.

$p<1$ tambÃ©m nÃ£o Ã© norma. Ex: $p=1/2$ falha na desigualdade triangular pois $(1,0)$ e $(0,1)$ tÃªm norma 1, mas a soma $(1,1)$ teria norma $2^{1/p}=4>2$. Apenas $1\le p\le \infty$ produz normas aceitÃ¡veis $\Vert v \Vert_p$ 
:::

## Exemplo

Calcular as normas 1, 2 e $\infty$ do vetor $x = \begin{bmatrix}2 &-3& 1\end{bmatrix}^\top$

$$ \Vert x\Vert_1 = \vert 2\vert + \vert -3\vert +\vert 1 \vert =6$$

$$ \Vert x\Vert_2 = \sqrt{\vert 2\vert^2 + \vert -3\vert^2 +\vert 1 \vert^2}= \sqrt{14} \approx 3,7417$$

$$ \Vert x\Vert_\infty = \max{\vert 2\vert + \vert -3\vert +\vert 1 \vert} =  3$$


## Pontos a igual distÃ¢ncia de $(0,0)$

As normas induzem geometrias distintas

![](figs/Aula06/norm_geometry.png) 

## Para quÃª normas diferentes?

- Ã€s vezes Ã© fÃ¡cil provar um resultado usando uma definiÃ§Ã£o de distÃ¢ncia, mas Ã© impossÃ­vel usando outra

  - Por exemplo, poderia ser fÃ¡cil mostrar que L1 ou L$_\infty$ de um vetor decresce para zero
  - Mas queremos mesmo Ã© que sua norma L2 vÃ¡ para zero
  - Mas isso serÃ¡ verdade!

## Para quÃª normas diferentes?{.scrollable}


- Todas as normas $\mathbb R^n$ sÃ£o equivalentes, ou seja, se $\Vert \cdot \Vert_\alpha$ e $\Vert \cdot \Vert_\beta$ sÃ£o normas no $\mathbb R^n$, entÃ£o existem constantes positivas $k_1$ e $k_2$, tais que
$$ k_1\Vert x\Vert_\alpha \le \Vert x \Vert_\beta \le k_2\Vert x\Vert_\alpha,$$
para todo $x\in \mathbb R^2$, particularmente,
 $$\Vert x\Vert_2 \le \Vert x\Vert_1 \le \sqrt n \Vert x\Vert_2$$

 $$ \Vert x \Vert_\infty \le \Vert x\Vert_2 \le \sqrt n \Vert x\Vert_\infty$$

 $$\Vert x\Vert_\infty \le \Vert x \Vert_1 \le n \Vert x \Vert_\infty$$

 - Se $\Vert x\Vert_1$ for pequena, $\Vert x \Vert_2$ serÃ¡ pequena tambÃ©m


## Propriedades com Lp

Normas Lp possuem as mesmas propriedades que vimos antes

- $\Vert x \Vert_p \ge 0 \text{ e } \Vert x \Vert_p =0 \Leftrightarrow x = 0$
- $\Vert \alpha x\Vert_p = \vert \alpha \vert \Vert x\Vert_p$ para todos escalares $\alpha$
- $\Vert x+y\Vert_p \le \Vert x \Vert_p + \Vert y \Vert_p$

<!-- Se $p>1$, $q>1$ com $\frac{1}{p} + \frac{1}{q} = 1$, entÃ£o

$$\vert x^\top y\vert \le \Vert x\Vert_p \Vert y\Vert_p$$ -->


<!-- ## Uma razÃ£o menos teÃ³rica

Vamos querer o menor vetor $(w_1, w_2)$ que pertenÃ§a a um certo conjunto
![](figs/Aula06/Norma_menor_vetor.png) 
-->